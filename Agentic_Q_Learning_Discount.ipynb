{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn7N9UOH1F3dY16dusf/IA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paviayyala/AIML-Lab/blob/main/Agentic_Q_Learning_Discount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working.\n",
        "\n",
        "**Agentic Q-Learning for Sales Discount — Colab Documentation**\n",
        "\n",
        "A clear, copy-ready Colab README that explains the agentic Q-learning program (tools + RuleBasedAgent) you shared. Paste this into a Colab text cell (Markdown) or save as README.md to keep with your notebook.\n",
        "\n",
        "Overview\n",
        "\n",
        "This project demonstrates an agentic structure for a simple tabular Q-learning agent that learns discount strategies for retail sales.\n",
        "It follows the pattern in your example:\n",
        "\n",
        "small environment (DiscountEnv) that simulates customer segments and competitor pressure,\n",
        "\n",
        "tools exposed as functions (generate_env, train_q_learning, evaluate_policy, save_policy, load_policy),\n",
        "\n",
        "a RuleBasedAgent orchestrator that dispatches tasks like train, evaluate, and train_and_eval.\n",
        "\n",
        "The reward is expected profit = (selling_price − cost) × purchase_probability. The agent learns when discounts improve expected profit vs. when to protect margin.\n",
        "\n",
        "\n",
        "Intended Use\n",
        "\n",
        "Train a Q-learning agent to choose discount percentages for different customer segments and competitor pressures.\n",
        "\n",
        "Save and load resulting Q tables (policies).\n",
        "\n",
        "Evaluate and inspect action distributions and expected profit.\n",
        "\n",
        "Run and iterate in Google Colab — recommended for interactive runs and saving artifacts to Google Drive."
      ],
      "metadata": {
        "id": "s2-WIvouefy_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWKpXCmUeL-X",
        "outputId": "b4bcc4d8-a25a-409c-a1b8-274f9d65af85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Summary ---\n",
            "Episodes: 1500\n",
            "Last 5 episode rewards (sample): [218.16, 277.53499999999997, 267.535, 265.295, 232.24000000000004]\n",
            "\n",
            "--- Evaluation Summary ---\n",
            "Average expected profit per trial: 24.7966\n",
            "Action distribution (discount_pct -> counts):\n",
            "  0% -> 389\n",
            "  5% -> 0\n",
            "  10% -> 1132\n",
            "  15% -> 0\n",
            "  20% -> 479\n",
            "\n",
            "Policy saved to: policies/discount_q_policy.json\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# agentic_q_discount.py\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import json\n",
        "import logging\n",
        "from typing import Tuple, Dict, Any, List\n",
        "\n",
        "# Optional: mimic the decorator you used (no langchain import required)\n",
        "def tool(func):\n",
        "    func.is_tool = True\n",
        "    func.name = func.__name__\n",
        "    return func\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# -------------- Environment --------------\n",
        "class DiscountEnv:\n",
        "    \"\"\"\n",
        "    Simple simulated environment for discount decision:\n",
        "    States: (customer_segment, competitor_pressure)\n",
        "      - customer_segment in {\"price_sensitive\", \"neutral\", \"loyal\"}\n",
        "      - competitor_pressure in {\"low\", \"high\"}\n",
        "    Actions: discount percentages [0,5,10,15,20]\n",
        "    Reward: expected profit = (selling_price - cost) * purchase_prob\n",
        "    Purchase probability increases with discount and differs by customer segment.\n",
        "    \"\"\"\n",
        "\n",
        "    segments = [\"price_sensitive\", \"neutral\", \"loyal\"]\n",
        "    comp_pressures = [\"low\", \"high\"]\n",
        "    actions = [0, 5, 10, 15, 20]  # discount percentages\n",
        "\n",
        "    def __init__(self, base_price=100.0, cost=60.0, seed: int = 42):\n",
        "        self.base_price = base_price\n",
        "        self.cost = cost\n",
        "        random.seed(seed)\n",
        "\n",
        "    def sample_state(self) -> Tuple[str, str]:\n",
        "        # sample a state (simulate a random incoming customer + market condition)\n",
        "        seg = random.choices(self.segments, weights=[0.4, 0.4, 0.2])[0]\n",
        "        cp = random.choices(self.comp_pressures, weights=[0.6, 0.4])[0]\n",
        "        return (seg, cp)\n",
        "\n",
        "    def step(self, state: Tuple[str, str], action_index: int) -> Tuple[float, dict]:\n",
        "        \"\"\"\n",
        "        Given a state and action index, return reward and info (no next state dependency in this simple sim).\n",
        "        \"\"\"\n",
        "        discount_pct = self.actions[action_index]\n",
        "        sell_price = self.base_price * (1 - discount_pct / 100.0)\n",
        "\n",
        "        # purchase probability model (sigmoid-like)\n",
        "        seg, cp = state\n",
        "        # base willingness by segment\n",
        "        seg_base = {\"price_sensitive\": 0.2, \"neutral\": 0.5, \"loyal\": 0.75}[seg]\n",
        "        # competitor pressure reduces purchase prob when high\n",
        "        cp_factor = 0.85 if cp == \"high\" else 1.0\n",
        "        # discount effect: more discount increases purchase probability\n",
        "        # map discount 0..20 to bump 0..0.5\n",
        "        discount_bump = (discount_pct / 20.0) * 0.5\n",
        "        purchase_prob = seg_base * cp_factor + discount_bump\n",
        "        # clamp\n",
        "        purchase_prob = max(0.0, min(1.0, purchase_prob))\n",
        "\n",
        "        expected_profit = (sell_price - self.cost) * purchase_prob\n",
        "\n",
        "        # reward: expected_profit (can be negative if price < cost)\n",
        "        return expected_profit, {\n",
        "            \"sell_price\": sell_price,\n",
        "            \"purchase_prob\": purchase_prob,\n",
        "            \"discount_pct\": discount_pct\n",
        "        }\n",
        "\n",
        "    def state_to_index(self, state: Tuple[str, str]) -> int:\n",
        "        seg_idx = self.segments.index(state[0])\n",
        "        cp_idx = self.comp_pressures.index(state[1])\n",
        "        return seg_idx * len(self.comp_pressures) + cp_idx\n",
        "\n",
        "    def index_to_state(self, idx: int) -> Tuple[str, str]:\n",
        "        seg_idx = idx // len(self.comp_pressures)\n",
        "        cp_idx = idx % len(self.comp_pressures)\n",
        "        return (self.segments[seg_idx], self.comp_pressures[cp_idx])\n",
        "\n",
        "    @property\n",
        "    def n_states(self) -> int:\n",
        "        return len(self.segments) * len(self.comp_pressures)\n",
        "\n",
        "    @property\n",
        "    def n_actions(self) -> int:\n",
        "        return len(self.actions)\n",
        "\n",
        "# -------------- Tools --------------\n",
        "@tool\n",
        "def generate_env(config: dict = None) -> DiscountEnv:\n",
        "    config = config or {}\n",
        "    env = DiscountEnv(base_price=config.get(\"base_price\", 100.0),\n",
        "                      cost=config.get(\"cost\", 60.0),\n",
        "                      seed=config.get(\"seed\", 42))\n",
        "    logging.info(\"Environment generated with base_price=%s cost=%s\", env.base_price, env.cost)\n",
        "    return env\n",
        "\n",
        "@tool\n",
        "def train_q_learning(env: DiscountEnv,\n",
        "                     episodes: int = 2000,\n",
        "                     alpha: float = 0.1,\n",
        "                     gamma: float = 0.95,\n",
        "                     epsilon_start: float = 1.0,\n",
        "                     epsilon_end: float = 0.05,\n",
        "                     epsilon_decay: float = 0.999) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Train a tabular Q-learning agent on the simple environment.\n",
        "    Returns dict with q_table and training metrics.\n",
        "    \"\"\"\n",
        "    n_s = env.n_states\n",
        "    n_a = env.n_actions\n",
        "    # Initialize Q-table\n",
        "    Q = [[0.0 for _ in range(n_a)] for _ in range(n_s)]\n",
        "\n",
        "    epsilon = epsilon_start\n",
        "    rewards_history: List[float] = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state = env.sample_state()\n",
        "        s_idx = env.state_to_index(state)\n",
        "        # single-step episode (stateless transitions) — treat each sample independently\n",
        "        # but we run many samples per episode for stability\n",
        "        ep_reward = 0.0\n",
        "        steps_per_episode = 10\n",
        "        for step in range(steps_per_episode):\n",
        "            # Epsilon-greedy\n",
        "            if random.random() < epsilon:\n",
        "                a = random.randrange(n_a)\n",
        "            else:\n",
        "                # pick argmax\n",
        "                a = max(range(n_a), key=lambda ai: Q[s_idx][ai])\n",
        "\n",
        "            reward, info = env.step(state, a)\n",
        "            ep_reward += reward\n",
        "\n",
        "            # As environment is single-step, next state is sampled anew\n",
        "            next_state = env.sample_state()\n",
        "            ns_idx = env.state_to_index(next_state)\n",
        "\n",
        "            # Q-learning update\n",
        "            best_next = max(Q[ns_idx])\n",
        "            Q[s_idx][a] += alpha * (reward + gamma * best_next - Q[s_idx][a])\n",
        "\n",
        "            # set state to next for next step\n",
        "            state = next_state\n",
        "            s_idx = ns_idx\n",
        "\n",
        "        # decay epsilon\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "        rewards_history.append(ep_reward)\n",
        "\n",
        "        if (ep + 1) % max(1, (episodes // 10)) == 0:\n",
        "            logging.info(\"Episode %d/%d: ep_reward=%.2f epsilon=%.3f\", ep+1, episodes, ep_reward, epsilon)\n",
        "\n",
        "    return {\"q_table\": Q, \"rewards\": rewards_history, \"params\": {\"episodes\": episodes, \"alpha\": alpha, \"gamma\": gamma}}\n",
        "\n",
        "@tool\n",
        "def evaluate_policy(env: DiscountEnv, q_table: List[List[float]], trials: int = 1000) -> dict:\n",
        "    total_reward = 0.0\n",
        "    action_counts = {a: 0 for a in env.actions}\n",
        "    for _ in range(trials):\n",
        "        state = env.sample_state()\n",
        "        s_idx = env.state_to_index(state)\n",
        "        # pick greedy action\n",
        "        a_idx = max(range(env.n_actions), key=lambda ai: q_table[s_idx][ai])\n",
        "        reward, info = env.step(state, a_idx)\n",
        "        total_reward += reward\n",
        "        action_counts[env.actions[a_idx]] += 1\n",
        "    avg_reward = total_reward / trials\n",
        "    logging.info(\"Evaluation over %d trials => avg_reward=%.4f\", trials, avg_reward)\n",
        "    return {\"avg_reward\": avg_reward, \"action_distribution\": action_counts}\n",
        "\n",
        "@tool\n",
        "def save_policy(q_table: List[List[float]], filename: str = \"q_policy.json\") -> str:\n",
        "    os.makedirs(\"policies\", exist_ok=True)\n",
        "    path = os.path.join(\"policies\", filename)\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(q_table, f)\n",
        "    logging.info(\"Saved policy to %s\", path)\n",
        "    return path\n",
        "\n",
        "@tool\n",
        "def load_policy(filename: str = \"q_policy.json\") -> List[List[float]]:\n",
        "    path = os.path.join(\"policies\", filename)\n",
        "    with open(path, \"r\") as f:\n",
        "        q_table = json.load(f)\n",
        "    logging.info(\"Loaded policy from %s\", path)\n",
        "    return q_table\n",
        "\n",
        "# -------------- Agent Orchestrator (Agentic style) --------------\n",
        "class RuleBasedAgent:\n",
        "    def __init__(self, tools):\n",
        "        # map name -> function\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "\n",
        "    def invoke(self, task: str, **kwargs):\n",
        "        \"\"\"\n",
        "        Very simple dispatcher: parse task keywords and call relevant tools.\n",
        "        Supported tasks:\n",
        "          - 'train': train q learning (returns training result)\n",
        "          - 'evaluate': evaluate saved or given policy\n",
        "          - 'train_and_eval': train then evaluate and save\n",
        "        \"\"\"\n",
        "        task_l = task.lower()\n",
        "\n",
        "        if \"train_and_eval\" in task_l or (\"train\" in task_l and \"eval\" in task_l):\n",
        "            env = self.tools[\"generate_env\"](kwargs.get(\"env_config\"))\n",
        "            train_res = self.tools[\"train_q_learning\"](\n",
        "                env,\n",
        "                episodes=kwargs.get(\"episodes\", 2000),\n",
        "                alpha=kwargs.get(\"alpha\", 0.1),\n",
        "                gamma=kwargs.get(\"gamma\", 0.95),\n",
        "                epsilon_start=kwargs.get(\"epsilon_start\", 1.0),\n",
        "                epsilon_end=kwargs.get(\"epsilon_end\", 0.05),\n",
        "                epsilon_decay=kwargs.get(\"epsilon_decay\", 0.999)\n",
        "            )\n",
        "            q = train_res[\"q_table\"]\n",
        "            save_path = self.tools[\"save_policy\"](q, kwargs.get(\"policy_name\", \"q_policy.json\"))\n",
        "            eval_res = self.tools[\"evaluate_policy\"](env, q, trials=kwargs.get(\"trials\", 1000))\n",
        "            return {\"train\": train_res, \"save_path\": save_path, \"eval\": eval_res}\n",
        "\n",
        "        if \"train\" in task_l:\n",
        "            env = self.tools[\"generate_env\"](kwargs.get(\"env_config\"))\n",
        "            train_res = self.tools[\"train_q_learning\"](\n",
        "                env,\n",
        "                episodes=kwargs.get(\"episodes\", 2000),\n",
        "                alpha=kwargs.get(\"alpha\", 0.1),\n",
        "                gamma=kwargs.get(\"gamma\", 0.95),\n",
        "                epsilon_start=kwargs.get(\"epsilon_start\", 1.0),\n",
        "                epsilon_end=kwargs.get(\"epsilon_end\", 0.05),\n",
        "                epsilon_decay=kwargs.get(\"epsilon_decay\", 0.999)\n",
        "            )\n",
        "            return {\"train\": train_res}\n",
        "\n",
        "        if \"evaluate\" in task_l:\n",
        "            env = self.tools[\"generate_env\"](kwargs.get(\"env_config\"))\n",
        "            if \"policy\" in kwargs:\n",
        "                q_tbl = kwargs[\"policy\"]\n",
        "            else:\n",
        "                q_tbl = self.tools[\"load_policy\"](kwargs.get(\"policy_name\", \"q_policy.json\"))\n",
        "            eval_res = self.tools[\"evaluate_policy\"](env, q_tbl, trials=kwargs.get(\"trials\", 1000))\n",
        "            return {\"eval\": eval_res}\n",
        "\n",
        "        return {\"error\": \"Task not recognized. Use 'train', 'evaluate' or 'train_and_eval'.\"}\n",
        "\n",
        "# -------------- Run example --------------\n",
        "if __name__ == \"__main__\":\n",
        "    tools = [generate_env, train_q_learning, evaluate_policy, save_policy, load_policy]\n",
        "    agent = RuleBasedAgent(tools)\n",
        "\n",
        "    # Example: train and evaluate, save policy\n",
        "    logging.info(\"Starting training+evaluation run\")\n",
        "    result = agent.invoke(\n",
        "        \"train_and_eval\",\n",
        "        env_config={\"base_price\": 120.0, \"cost\": 70.0, \"seed\": 123},\n",
        "        episodes=1500,\n",
        "        alpha=0.08,\n",
        "        gamma=0.95,\n",
        "        epsilon_start=1.0,\n",
        "        epsilon_end=0.05,\n",
        "        epsilon_decay=0.998,\n",
        "        trials=2000,\n",
        "        policy_name=\"discount_q_policy.json\"\n",
        "    )\n",
        "\n",
        "    # Print summary\n",
        "    train_info = result[\"train\"]\n",
        "    eval_info = result[\"eval\"]\n",
        "    save_path = result[\"save_path\"]\n",
        "    print(\"\\n--- Training Summary ---\")\n",
        "    print(\"Episodes:\", train_info[\"params\"][\"episodes\"])\n",
        "    print(\"Last 5 episode rewards (sample):\", train_info[\"rewards\"][-5:])\n",
        "    print(\"\\n--- Evaluation Summary ---\")\n",
        "    print(\"Average expected profit per trial: {:.4f}\".format(eval_info[\"avg_reward\"]))\n",
        "    print(\"Action distribution (discount_pct -> counts):\")\n",
        "    for d, cnt in sorted(eval_info[\"action_distribution\"].items()):\n",
        "        print(f\"  {d}% -> {cnt}\")\n",
        "    print(\"\\nPolicy saved to:\", save_path)\n",
        "    print(\"Done.\")\n"
      ]
    }
  ]
}