{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4bPfEPK4LDz56OPtn7giY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paviayyala/AIML-Lab/blob/main/RL_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnsGEpIzSQVR",
        "outputId": "a827d52b-a0f3-46eb-c48f-04a3ce108341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode   1: state=High, action=0%, reward=80.0\n",
            "Current Q-table:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [8. 0. 0. 0.]]\n",
            "--------------------------------------------------\n",
            "Episode 100: state=Medium, action=0%, reward=30.0\n",
            "Current Q-table:\n",
            "[[ 67.06  20.15   8.    27.27]\n",
            " [ 79.57  36.13   0.    19.95]\n",
            " [155.13  40.58  18.31   5.04]]\n",
            "--------------------------------------------------\n",
            "Episode 200: state=Low, action=10%, reward=25.0\n",
            "Current Q-table:\n",
            "[[128.39  42.88   8.    75.14]\n",
            " [169.25  36.13  31.38 102.29]\n",
            " [208.45  64.16  31.39  47.45]]\n",
            "--------------------------------------------------\n",
            "Episode 300: state=Medium, action=0%, reward=30.0\n",
            "Current Q-table:\n",
            "[[200.03  94.66  49.01  89.45]\n",
            " [220.24  85.49  54.49 122.85]\n",
            " [262.01  99.75  75.85  63.1 ]]\n",
            "--------------------------------------------------\n",
            "Episode 400: state=Low, action=0%, reward=10.0\n",
            "Current Q-table:\n",
            "[[242.39  94.66 121.42 125.44]\n",
            " [253.61 122.68  80.33 145.5 ]\n",
            " [300.43 135.12 114.08 115.83]]\n",
            "--------------------------------------------------\n",
            "Episode 500: state=Medium, action=0%, reward=30.0\n",
            "Current Q-table:\n",
            "[[266.24 135.07 121.42 125.44]\n",
            " [276.89 159.85 102.09 193.88]\n",
            " [323.4  187.61 147.03 133.68]]\n",
            "--------------------------------------------------\n",
            "\n",
            "‚úÖ Final Q-table (States as rows, Actions as columns):\n",
            "Low    -> 0%:266.24, 10%:135.07, 20%:121.42, 30%:125.44\n",
            "Medium -> 0%:276.89, 10%:159.85, 20%:102.09, 30%:193.88\n",
            "High   -> 0%:323.40, 10%:187.61, 20%:147.03, 30%:133.68\n",
            "\n",
            "üèÅ Learned Optimal Policy (what the agent learned):\n",
            "  When demand is Low    ‚Üí offer 0%\n",
            "  When demand is Medium ‚Üí offer 0%\n",
            "  When demand is High   ‚Üí offer 0%\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# üß† Simple Q-Learning Example\n",
        "# ------------------------------------------------------------\n",
        "# Concept: The agent learns which DISCOUNT to offer based on DEMAND.\n",
        "# States represent demand levels (Low, Medium, High).\n",
        "# Actions represent possible discounts (0%, 10%, 20%, 30%).\n",
        "# The goal: maximize the long-term reward (e.g., sales/profit).\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Format how numbers are printed (for cleaner tables)\n",
        "np.set_printoptions(precision=2, suppress=True)\n",
        "\n",
        "# ============================================================\n",
        "# 1Ô∏è‚É£ DEFINE STATES AND ACTIONS\n",
        "# ------------------------------------------------------------\n",
        "# The agent can see one of three states (demand levels)\n",
        "# and can choose one of four possible discounts.\n",
        "# ============================================================\n",
        "\n",
        "states = [\"Low\", \"Medium\", \"High\"]              # indices: 0, 1, 2\n",
        "actions = [\"0%\", \"10%\", \"20%\", \"30%\"]           # indices: 0, 1, 2, 3\n",
        "\n",
        "n_states = len(states)   # number of possible demand levels\n",
        "n_actions = len(actions) # number of possible discount actions\n",
        "\n",
        "# ============================================================\n",
        "# 2Ô∏è‚É£ INITIALIZE Q-TABLE\n",
        "# ------------------------------------------------------------\n",
        "# Q[s, a] will store how \"good\" it is to take action a in state s.\n",
        "# It starts with 0 because the agent knows nothing at first.\n",
        "# ============================================================\n",
        "\n",
        "Q = np.zeros((n_states, n_actions), dtype=float)\n",
        "\n",
        "# ============================================================\n",
        "# 3Ô∏è‚É£ DEFINE REWARD TABLE (ENVIRONMENT)\n",
        "# ------------------------------------------------------------\n",
        "# This simulates the environment feedback.\n",
        "# Each cell represents the IMMEDIATE REWARD (e.g., profit or sales)\n",
        "# if the agent takes a particular action in a particular state.\n",
        "# Higher reward = better outcome.\n",
        "# ============================================================\n",
        "\n",
        "reward_table = np.array([\n",
        "    # Actions: 0%   10%   20%   30%\n",
        "    [10,   25,   45,   40],   # Low demand\n",
        "    [30,   55,   65,   50],   # Medium demand\n",
        "    [80,   75,   60,   45],   # High demand\n",
        "], dtype=float)\n",
        "\n",
        "# ============================================================\n",
        "# 4Ô∏è‚É£ Q-LEARNING PARAMETERS (HYPERPARAMETERS)\n",
        "# ------------------------------------------------------------\n",
        "# Œ± (alpha): Learning rate ‚Äî how fast the agent updates old beliefs\n",
        "# Œ≥ (gamma): Discount factor ‚Äî how much future rewards matter\n",
        "# Œµ (epsilon): Exploration rate ‚Äî how often to try something new\n",
        "# episodes: Number of training iterations (the more, the better)\n",
        "# ============================================================\n",
        "\n",
        "alpha = 0.1       # learning rate\n",
        "gamma = 0.9       # discount factor\n",
        "epsilon = 0.2     # exploration probability (20% explore, 80% exploit)\n",
        "episodes = 500    # number of learning rounds\n",
        "print_every = 100 # how often to print progress\n",
        "\n",
        "# ============================================================\n",
        "# 5Ô∏è‚É£ TRAINING LOOP (THE HEART OF Q-LEARNING)\n",
        "# ------------------------------------------------------------\n",
        "# In each episode:\n",
        "#   1. Pick a random state (like \"today's market condition\").\n",
        "#   2. Choose a discount action using Œµ-greedy policy.\n",
        "#   3. Receive a reward (sales result).\n",
        "#   4. Move to a new random state (simulated next day).\n",
        "#   5. Update the Q-value using the Q-learning formula.\n",
        "# ------------------------------------------------------------\n",
        "# Formula:\n",
        "# Q(s,a) ‚Üê Q(s,a) + Œ± [r + Œ≥ * max(Q(s‚Äô,¬∑)) ‚àí Q(s,a)]\n",
        "# ============================================================\n",
        "\n",
        "for ep in range(1, episodes + 1):\n",
        "    # Pick a random initial state\n",
        "    s = np.random.randint(n_states)\n",
        "\n",
        "    # ------------------------------\n",
        "    # Step 1: Choose an action\n",
        "    # ------------------------------\n",
        "    # With probability Œµ, explore (try a random discount)\n",
        "    # Otherwise, exploit (choose the best-known discount)\n",
        "    if np.random.rand() < epsilon:\n",
        "        a = np.random.randint(n_actions)   # Explore: try something random\n",
        "    else:\n",
        "        a = np.argmax(Q[s])                # Exploit: choose best action so far\n",
        "\n",
        "    # ------------------------------\n",
        "    # Step 2: Get immediate reward\n",
        "    # ------------------------------\n",
        "    r = reward_table[s, a]                 # What we earned today (reward)\n",
        "\n",
        "    # ------------------------------\n",
        "    # Step 3: Simulate next state\n",
        "    # ------------------------------\n",
        "    # In real life, next state depends on environment changes.\n",
        "    # Here we simplify by picking a random new demand level.\n",
        "    s_next = np.random.randint(n_states)\n",
        "\n",
        "    # ------------------------------\n",
        "    # Step 4: Q-value update\n",
        "    # ------------------------------\n",
        "    best_next_q = np.max(Q[s_next])        # Best possible future value\n",
        "    td_target = r + gamma * best_next_q    # Target value (reward + discounted future)\n",
        "    td_error = td_target - Q[s, a]         # Difference between new & old estimate\n",
        "    Q[s, a] += alpha * td_error            # Update the Q-value slightly\n",
        "\n",
        "    # ------------------------------\n",
        "    # Step 5: Print progress occasionally\n",
        "    # ------------------------------\n",
        "    if ep % print_every == 0 or ep == 1:\n",
        "        print(f\"Episode {ep:3d}: state={states[s]}, action={actions[a]}, reward={r:.1f}\")\n",
        "        print(\"Current Q-table:\")\n",
        "        print(Q)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# ============================================================\n",
        "# 6Ô∏è‚É£ SHOW FINAL RESULTS\n",
        "# ------------------------------------------------------------\n",
        "# After training, the Q-table contains learned values.\n",
        "# The highest Q-value in each row shows the best discount for that demand level.\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n‚úÖ Final Q-table (States as rows, Actions as columns):\")\n",
        "for i, st in enumerate(states):\n",
        "    row = Q[i]\n",
        "    row_display = \", \".join(f\"{actions[j]}:{row[j]:.2f}\" for j in range(n_actions))\n",
        "    print(f\"{st:6s} -> {row_display}\")\n",
        "\n",
        "# ============================================================\n",
        "# 7Ô∏è‚É£ LEARNED POLICY (GREEDY STRATEGY)\n",
        "# ------------------------------------------------------------\n",
        "# The agent now simply picks the discount with the highest Q-value for each state.\n",
        "# This represents what it has learned after many trials.\n",
        "# ============================================================\n",
        "\n",
        "policy = [actions[np.argmax(Q[i])] for i in range(n_states)]\n",
        "\n",
        "print(\"\\nüèÅ Learned Optimal Policy (what the agent learned):\")\n",
        "for st, act in zip(states, policy):\n",
        "    print(f\"  When demand is {st:6s} ‚Üí offer {act}\")\n"
      ]
    }
  ]
}